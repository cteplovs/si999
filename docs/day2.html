<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>Big Data Workshop</title>

  <link rel="stylesheet" href="css/reveal.css">
  <link rel="stylesheet" href="css/theme/um1.css">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <!-- Printing and PDF exports -->
  <script>
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!-- https://github.com/hakimel/reveal.js/issues/174 -->
  <style>
  .slides .header {
    top: 0px;
    right: 0px;
  }
  .slides .footer {
    position: absolute;
    bottom: 0px;
    left: 20%;
    text-align: center;
    font-size: small;
  }
  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">
      <div class="footer">
        <small><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</small>
      </div>
      <section>
        <h2>Big Data, Predictive Analytics and Deep Learning with Apache Spark</h2>
        <h4>Chris Teplovs, Ph.D.</h4>
        <h5>Day 2</h5>
      </section>

      <section>
        <section>
          <h2>Workshop overview</h2>
          <table>
            <tr><td>Day 1:<br/> Focus on data</td><td>Introductions to each other,
              the workshop, Big Data, Spark and Databricks</td></tr>
              <tr>
                <td>Day 2:<br/> Focus on techniques</td>
                <td>Clustering, classification and analytic pipelines</td></tr>
              <tr><td>Day 3: <br/>Focus on the future</td><td>Deep Learning, Neural networks, and project presentations</td></tr>
            </table>
          </section>
          <section>
            <h2>Day 1 (yesterday)</h2>
            <table>
              <tr> <th>Segment</th><th>Topic</th> </tr>
              <tr> <td>1.1</td><td>Workshop overview and Introductions</td> </tr>
              <tr> <td>1.2</td><td>Introduction to Databricks</td> </tr>
              <tr> <td>1.3</td><td>Hands-On: Databricks</td> </tr>
              <tr> <td>1.4</td><td>Intro to Spark & DataFrames</td> </tr>
              <tr> <td>1.5</td><td>Hands-On: DataFrames</td> </tr>
              <tr> <td>1.6</td><td>Big Data Sets</td></tr>
              <tr> <td>1.7</td><td>Hands-On: Exploring Data</td></tr>
            </table>
          </section>
          <section>
            <h2>Day 2 (today)</h2>
            <table>
              <tr> <th>Segment</th><th>Topic</th> </tr>
              <tr> <td>2.1</td><td>Clustering Overview</td> </tr>
              <tr> <td>2.2</td><td>k-Means and Bisecting k-Means</td> </tr>
              <tr> <td>2.3</td><td>Hands-On: Clustering</td> </tr>
              <tr> <td>2.4</td><td>Classification Overview</td> </tr>
              <tr> <td>2.5</td><td>Hands-On: Classification</td> </tr>
              <tr> <td>2.6</td><td>Model Evaluation and Tuning</td></tr>
              <tr> <td>2.7</td><td>Hands-On: Evaluation and Tuning</td></tr>
            </table>
          </section>

        </section>
          <section>
            <h2>Clustering</h2>
          </section>

          <section>
            <h2>Cluster analysis</h2>
            <ul>
              <li>finds "interesting" groups of objects based on similarity</li>
              <li>what typically makes a "good" clustering?
                <ul>
                  <li>members are highly similar to each other (i.e. minimize within-cluster distances)</li>
                  <li>clusters are well-separated from each other (i.e. maximize between-cluster distances)</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <h2>A "good" clustering solution</h2>
            <img src="assets/clustering-good.png"/>
          </section>
          <section>
            <h2>Applications of Cluster Analysis</h2>
            <ul>
              <li>understanding
              <ul>
                <li>group related documents for browsing</li>
                <li>group genes and proteins that have similar functionality</li>
                <li>group stocks with similar price fluctuations</li>
              </ul>
            </li>
            <li>summarization
              <ul>
                <li>reduce size of large data sets</li>
              </ul>
            </li>
            </ul>
          </section>
          <section>
            <h2>Cluster Analysis Workflow</h2>
            <ol>
              <li>Formulate the problem</li>
              <li>Select a distance measure (optional)</li>
              <li>Select a clustering procedure</li>
              <li>Decide on number of clusters</li>
              <li>Interpret and profile clusters</li>
              <li>Assess validity of clustering</li>
            </ol>
          </section>
          <section>
            <h2>Clustering: useful in exploratory data analysis</h2>
            <ol>
              <li>Data understanding: finding underlying factors, groups, structure</li>
              <li>Data navigation: web search and browsing</li>
              <li>Data reduction: create new nominal variables</li>
              <li>Data smoothing: infer missing attributes from cluster neighbors</li>
            </ol>
          </section>
          <section>
            <h2>Clustering arises in many fields</h2>
            <ul>
              <li>Health
                <ul>
                  <li>DNA gene expression (e.g. cancer, immunomarkers)</li>
                  <li>Medical imaging
                </ul>
              </li>
              <li>Business
                <ul>
                  <li>Market segments</li>
                  <li>Web site visitors</li>
                </ul>
              </li>
              <li>Social Network analysis
                <ul>
                  <li>Find communities</li>
                </ul>
              </li>
              <li>Information retrieval
              <ul>
                <li>searh results clustered by similarity</li>
                <li>personalization for groups of similar users</li>
              </ul>
              </li>
              <li>Speech understanding
                <ul>
                  <li>convert waveforms to categories</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <section>
              <h2>Finding the "best" clustering</h2>
              <img src="assets/clustering-how-many.png" alt="how many clusters?">
            </section>
            <section>
              <img src="assets/clustering-2.png" alt="">
            </section>
            <section>
              <img src="assets/clustering-4.png" alt="">
            </section>
            <section>
              <img src="assets/clustering-6.png" alt="">
            </section>
            <section>
              <img src="assets/clustering-246.png" alt="">
            </section>
          </section>
          <section>
            <h2>Clustering algorithms</h2>
            <ul>
              <li>hard (objects belong to only 1 cluster) vs. soft (multiple membership)</li>
              <li>hierarchical vs. non-hierarchical (flat)</li>
              <li>agglomerative vs. divisive</li>
            </ul>
            <p>We will focus on flat and hierarchical divisive methods:
              k-means and bisecting k-means</p>
          </section>
          <section>
            <h2>FYI: Agglomerative Hierarchical</h2>
            <ul>
              <li>produces a set of nested clusters organized as a hierarchical tree</li>
              <li>can be visualized as a dendrogram:</li>
            </ul>
          </section>
          <section>
            <img src="assets/clustering-sahn.png" alt="">
          </section>
          <section>
            <h2>k-means clustering</h2>
            <ul>
              <li>divisive clustering</li>
              <li>each cluster has a centroid (center point)</li>
              <li>each point is  assigned to its nearest centroid</li>
              <li>number of clusters (k) is specified in advance</li>
            </ul>
          </section>
          <section>
            <h2>k-means algorithm</h2>
            <img src="assets/k-means-algorithm.png" alt="">
          </section>
          <section>
            <h2>k-means iterations</h2>
            <img src="assets/k-means-iterations.png" alt="">
          </section>
          <section>
            <h2>k-means notables</h2>
            <ul>
              <li>different initializations of centroids can yield different results</li>
              <li>centroid is typically the mean of the points in the cluster (c.f. medoids)</li>
              <li>proximity can be measured by Euclidean distance, cosine similarity, correlation, Manhattan distance, etc.</li>
              <li>computationally complex (relatively speaking)</li>
            </ul>
          </section>
          <section>
            <h2>Limitations of k-means</h2>
            <ul>
              <li>k-means has problems when clusters are of differing sizes, densities, or are "oddly" shaped</li>
              <li>outliers can cause problems</li>
            </ul>
          </section>
          <section>
            <h2>Bisecting k-means</h2>
            <ul>
              <li>hierarchical divisive technique</li>
              <li>uses k-means with k=2 at each iteration</li>
            </ul>
          </section>
          <section>
            <h2>Bisecting k-means</h2>
            <pre>
Loop: until the stopping condition for the number of Clusters has
      been reached
     Loop: for every cluster         
        - Measure the total error for the parent cluster in this
          loop's iteration
        - Apply K-Means Algorithm to the cluster with k=2
        - Measure the total SSE error of the children clusters
          compared to their parent cluster         
        - Choose the cluster split that gives the lowest error and
          commit this split     
     End Loop
End Loop
</pre>
          </section>
          <section>
            <h2>Relative Merits of Bisecting k-means</h2>
            <ul>
              <li>computationally efficient (k-2)</li>
              <li>resulting clusters tend to be stable</li>

            </ul>
            <p>However. tends to produce different clusters than k-means.</p>
          </section>
          <section>
            <h2>Clustering in Spark</h2>
            <ul>
              <li>Why spark?</li>
              <li>Why not scikit-learn?</li>
              <li>Why not R?</li>
              <li>Why focus on k-means and bisecting k-means?</li>
            </ul>
          </section>
          <section>
            <h2>How many clusters?</h2>
            <ul>
              <li>Theoretical, conceptual or practical issues many suggest
              a certain number of clusters</li>
              <li>ratio of total within-group variance to between-group variance
              vs. number of clusters</li>
              <li>look for "elbow" in resulting plot</li>
            </ul>
          </section>
          <section>
            <h2>How many clusters?</h2>
            <img src="assets/clustering-scree.png" alt="">
          </section>
          <section>
            <h2>Good clusters?</h2>
            <ul>
              <li>stable across purturbations (different methods, e.g. distance metrics)</li>
              <li>silhouette score (1 = good, -1 = really bad)</li>
            </ul>
          </section>
          <section>
            <h2>Silhouette plot</h2>
            <img src="assets/silhouette-plot.png" alt="">
          </section>
          <section>
		  <h2><a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1101947090054953/3411870455190903/6387199866535941/latest.html">To the notebook!</a></h2>
          </section>
<section>
  <h2>Break!</h2>
</section>
<!-- LUNCH -->
<section>
  <h2>Classification</h2>
  <ul>
    <li>classification and classification types</li>
    <li>algorithms
<ul>
  <li>Naive Bayes</li>
  <li>Decision Tree</li>
  <li>Random Forest</li>
</ul>
    </li>
    <li>Evaluation: Train/Test and Cross-Validation</li>
  </ul>
</section>
<section>
  <h2>Clustering vs. Classification</h2>
  <ul>
    <li>With clustering, we knew there was structure
      (e.g. different types of people, etc.),
      but we didn't know what the structure was</li>
      <li>clustering is unsupervised</li>
      <li>goal: find the structure</li>
      <li>usually don't know which things go together in a cluster</li>
      <li>may or may not know how many clusters</li>
      <li>usually figure out what clusters mean after the fact</li>
  </ul>
</section>
<section>
  <h2>Clustering vs. Classification</h2>
  <ul>
    <li>Classification:</li>
    <li>often supervised (or semi-supervised)</li>
    <li>we know the labels of things (e.g. spam vs. non-spam, pop vs. classical)</li>
    <li>computer "learns" rule(s) of where to put things</li>
    <li>we don't know which features are best predictors of membership</li>
    <li>usually know which things go together in a cluster</li>
    <li>usually know how many clusters there are</li>
    <li>usually know what the clusters "mean" in advance</li>
  </ul>
</section>
<section>
  <h2>Classification is about...</h2>
  <ul>
    <li>Answering or predicting something (Y) given an input (X):</li>
    <li>Is X a Y (or not)? (e.g. is this email spam or not?)</li>
    <li>What group (Y) does X belong to? (e.g. is this a forest or a mountain?)</li>
    <li>What is the value of Y given X? (e.g. what grade should this student get?)</li>
  </ul>
</section>
<section>
  <h2>Classifiers work by...</h2>
  <p>Being fed examples and learning how important certain features arises</p>
</section>
<section>
  <h2>Classification workflow</h2>
  <ol>
    <li>Generate/obtain labels</li>
    <li>Generate/obtain features</li>
    <li>Select a classifier</li>
    <li>Train classifier</li>
    <li>Tune classifier</li>
    <li>Test classifier</li>
  </ol>
</section>
<section>
  <h2>Getting labels</h2>
  <ul>
    <li>Painful, expensive, time-consuming</li>
    <li>Often human labor</li>
    <li>Can infer labels (e.g. predict gender by examining name of author)</li>
    <li>Synthetic datasets</li>
  </ul>
</section>
<section>
  <h2>Features</h2>
  <ul>
    <li>Same ideas as in clustering</li>
    <li>Some set of "descriptions" for an object: explicit and
      inferred (calculated)</li>

  </ul>
</section>
<section>
  <h2>Clustering vs. Classification</h2>
  <ul>
    <li>Clustering tries to separate groups by using (dis)similarity</li>
    <li>Classification tries to find important features for
      distinguishing group membership</li>
  </ul>
</section>
<section>
  <h2>Some popular classifiers</h2>
  <ul>
    <li>k-Nearest Neighbor (kNN)</li>
    <li>Logistic Regression</li>
    <li>Naive Bayes</li>
    <li>Decision Tree</li>
    <li>Random forest</li>
    <li>SVM</li>
    <li>Neural Networks ("deep learning")</li>
  </ul>
</section>
<section>
  <h2>Our focus: Decision Trees and Random Forests</h2>
</section>
<section>
  <h2>Decision Trees</h2>
  <p>I'm thinking of an animal: ask me some yes/no questions.</p>
</section>
<section>
  <h2>Decision Trees</h2>
  <ul>
    <li>Ask the question with the most valuable answer first</li>
    <li>"If I knew the answer to this,
      how much closer to the solution would I be?"</li>
    <li>Solutions that divide the space 50/50 are better than
      solutions that divide the space 98/2</li>
  </ul>
</section>
<section>
  <h2>INSERT DT VIS HERE</h2>
</section>
<section>
  <h2>Decision Tree Advantages</h2>
  <ul>
    <li>easy to Interpret</li>
    <li>prediction process is obvious</li>
    <li>can handle mixed data types</li>
  </ul>
</section>
<section>
  <h2>Decision Tree Limitations</h2>
  <ul>
    <li>expensive to calculate</li>
    <li>tendency to overfit</li>
    <li>can get large</li>
  </ul>
</section>
<section>
  <h2>Random Forest</h2>
  <ul>
    <li>currently a favorite technique</li>
    <li>can fix the problem of one tree by using many</li>
    <li>various ways to randomize: pick different subsets of data,
    pick different features</li>
  </ul>
</section>
<section>
  <h2>How do you visualize a random forest?</h2>
  <img src="assets/bob-ross-forest.png" alt="">
</section>
<section>
  <h2>Classification Summary</h2>
  <ul>
    <li>ubiquitous</li>
    <li>possibly dangerous</li>
    <li>good for when we know somerthing about the structure</li>
    <li>same type of "pipeline" as clustering</li>
  </ul>
</section>
<section>
	<h2><a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1101947090054953/3411870455190903/6387199866535941/latest.html">To the notebook!</a></h2>
</section>
<section>
  <h3><a href="index.html">Workshop Overview</a></h3>
  <h3><a href="day1.html">Day 1</a></h3>
  <h3><a href="day2.html">Day 2</a></h3>
  <h3><a href="day3.html">Day 3</a></h3>
</section>
      </div> <!-- slides -->
    </div> <!-- reveal -->

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
    // More info about config & dependencies:
    // - https://github.com/hakimel/reveal.js#configuration
    // - https://github.com/hakimel/reveal.js#dependencies
    Reveal.initialize({
      transition: 'none', // none/fade/slide/convex/concave/zoom
      dependencies: [
        { src: 'plugin/markdown/marked.js' },
        { src: 'plugin/markdown/markdown.js' },
        { src: 'plugin/notes/notes.js', async: true },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
      ]
    });
    </script>
  </body>
  </html>
