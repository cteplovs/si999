{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data, Predictive Analytics and Deep Learning Workshop\n",
    "## Notebook #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to \"mount\" an Amazon Web Services (AWS) S3 bucket.  Think of this as a virtual USB key drive:  \n",
    "whatever is in that bucket will be available to your Databricks notebook via the path \"/mnt/umsi-data-science\".   The following cell\n",
    "mounts the S3 bucket that I've set up for this workshop.\n",
    "\n",
    "**NOTE: You will need to change the value of SECRET_KEY to the value that we sent you via email.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following line\n",
    "SECRET_KEY = \"\"\n",
    "\n",
    "# DO NOT change the following lines\n",
    "ACCESS_KEY = \"\"\n",
    "\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"umsi-data-science-west\"\n",
    "MOUNT_NAME = \"umsi-data-science\"\n",
    "try:\n",
    "  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\n",
    "except:\n",
    "  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\n",
    "dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n",
    "print(\"%s has been mounted.\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/umsi-data-science/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/mnt/umsi-data-science/data/yelp/business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"businesses\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM businesses\")\n",
    "sqlDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT state,count(*) as c FROM businesses where attributes.ambience.hipster group by state order by c desc\" )\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "name": "Notebook #2",
  "notebookId": 2813453018090786
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
