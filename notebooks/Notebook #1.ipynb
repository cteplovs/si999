{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data, Predictive Analytics and Deep Learning Workshop\n",
    "## Notebook #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to keep things real, let's try a simple python command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, big data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_other_list = spark.createDataFrame(\n",
    "    [('Chris',67),('Frank',70)], [â€˜name','score'])\n",
    "df_from_other_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "df_from_list = spark.createDataFrame(\n",
    "    [1.0,2.0,3.0,4.0,5.0], FloatType())\n",
    "df_from_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('Chris',[67,42]),('Frank',[70,72])],['name','scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('score',explode('scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('good', F.when(df['score'] > 50,1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to \"mount\" an Amazon Web Services (AWS) S3 bucket.  Think of this as a virtual USB key drive:  \n",
    "whatever is in that bucket will be available to your Databricks notebook via the path \"/mnt/umsi-data-science\".   The following cell\n",
    "mounts the S3 bucket that I've set up for this workshop.\n",
    "\n",
    "**NOTE: You will need to change the value of SECRET_KEY to the value that we sent you via email.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following line\n",
    "SECRET_KEY = \"\"\n",
    "\n",
    "# DO NOT change the following lines\n",
    "ACCESS_KEY = \"\"\n",
    "\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"umsi-data-science-west\"\n",
    "MOUNT_NAME = \"umsi-data-science\"\n",
    "try:\n",
    "  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\n",
    "except:\n",
    "  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\n",
    "dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n",
    "print(\"%s has been mounted.\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/umsi-data-science/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/mnt/umsi-data-science/data/yelp/business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"businesses\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM businesses\")\n",
    "sqlDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT state,count(*) as c FROM businesses where attributes.ambience.hipster group by state order by c desc\" )\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "name": "Notebook #1",
  "notebookId": 1939366193503920
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
