{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Big Data, Predictive Analytics and Deep Learning Workshop\n## Notebook #1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just to keep things real, let's try a simple python command:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Hello, big data!\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_from_other_list = spark.createDataFrame(\n    [('Chris',67),('Frank',70)], [â€˜name','score'])\ndf_from_other_list.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import FloatType\ndf_from_list = spark.createDataFrame(\n    [1.0,2.0,3.0,4.0,5.0], FloatType())\ndf_from_list.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame([('Chris',[67,42]),('Frank',[70,72])],['name','scores'])"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import explode\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = df.withColumn('score',explode('scores'))"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark.sql.functions as F"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.withColumn('good', F.when(df['score'] > 50,1).otherwise(0)).show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we're going to \"mount\" an Amazon Web Services (AWS) S3 bucket.  Think of this as a virtual USB key drive:  \nwhatever is in that bucket will be available to your Databricks notebook via the path \"/mnt/umsi-data-science\".   The following cell\nmounts the S3 bucket that I've set up for this workshop.\n\n**NOTE: You will need to change the value of SECRET_KEY to the value that we sent you via email.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Change the following line\nSECRET_KEY = \"\"\n\n# DO NOT change the following lines\nACCESS_KEY = \"\"\n\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"umsi-data-science-west\"\nMOUNT_NAME = \"umsi-data-science\"\ntry:\n  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\nexcept:\n  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\nprint(\"%s has been mounted.\" % MOUNT_NAME)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display(dbutils.fs.ls(\"/mnt/umsi-data-science/data\"))"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = spark.read.json(\"/mnt/umsi-data-science/data/yelp/business.json\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.printSchema()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"businesses\")\n\nsqlDF = spark.sql(\"SELECT * FROM businesses\")\nsqlDF.show()\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sqlDF = spark.sql(\"SELECT state,count(*) as c FROM businesses where attributes.ambience.hipster group by state order by c desc\" )\nsqlDF.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
